{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_id": "cf5385c08cfa4808b8beff80560e22b1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7236,
    "execution_start": 1676554365304,
    "source_hash": "2ad234dc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from time import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from torch.nn import ReLU\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cell_id": "cae045036094459a865ca94e67d08f3d",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4529,
    "execution_start": 1676461535706,
    "source_hash": "ac825e63",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "{'num_workers': 1, 'pin_memory': True}\n",
      "C:\\Users\\josep\\Dropbox\\Joseph\\AgroParisTech\\IODAAAAA\\Projet Fil Rouge\\PFR_Cirses_2022_local_run\\data\n"
     ]
    }
   ],
   "source": [
    "#setting up all the parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True, batch_size = 128} if torch.cuda.is_available() else {}\n",
    "print(kwargs)\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\josep\\Dropbox\\Joseph\\AgroParisTech\\IODAAAAA\\Projet Fil Rouge\\PFR_Cirses_2022_local_run\\data\"\n",
    "print(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish with:2.838589906692505 second, num_workers=2\n",
      "Finish with:3.088090658187866 second, num_workers=4\n",
      "Finish with:3.6544947624206543 second, num_workers=6\n",
      "Finish with:4.4289634227752686 second, num_workers=8\n",
      "Finish with:5.138800382614136 second, num_workers=10\n"
     ]
    }
   ],
   "source": [
    "#benchmarking the num workers \n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "for num_workers in range(2, mp.cpu_count(), 2):  \n",
    "    train_loader = DataLoader(DATA_PATH,shuffle=True, num_workers=num_workers ,batch_size=128,pin_memory=True)\n",
    "    start = time()\n",
    "    for epoch in range(1, 3):\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            pass\n",
    "    end = time()\n",
    "    print(\"Finish with:{} second, num_workers={}\".format(end - start, num_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_id": "fe4651b9a1a44deea3d24004238ec124",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1676554391740,
    "source_hash": "338478ad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataSplit:\n",
    "    \"\"\"\n",
    "    Replace DataLoader !\n",
    "    Allows to triple Load the data into three differents sets : train, test and val\n",
    "    The size of the datasets depends on the splits inputed as train_split, val_split and test_split\n",
    "    It is possible to have the data randomly sampled with the argument shuffle = True\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=False):\n",
    "        \"\"\"\n",
    "        Initialize the data split based on the splits given in inputs\n",
    "        Inputs : \n",
    "            - dataset : dataset loaded with torchivison.datasets.ImageFolder()\n",
    "            - train_split, val_split and test_split : the size of the splits in % of the dataset size\n",
    "            - shuffle : the possibility of sampling randomly through the dataset\n",
    "        \"\"\"\n",
    "        self.batch_size=128\n",
    "        self.num_workers=2\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        dataset_size = len(dataset)\n",
    "        self.indices = list(range(dataset_size)) #all the indices\n",
    " \n",
    "        #the quantity of data in each section of dataset\n",
    "        train_split = int(np.floor(train_split * dataset_size))\n",
    "        val_split = int(np.floor(val_split * dataset_size))\n",
    "        #test split is what is left not sampled\n",
    "        test_split = int(np.floor(dataset_size - train_split - val_split))\n",
    "    \n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "        self.train_indices = self.indices[:train_split]\n",
    "        self.val_indices = self.indices[train_split : train_split+val_split]\n",
    "        self.test_indices = self.indices[train_split + val_split :]\n",
    "\n",
    "        self.train_sampler = SubsetRandomSampler(self.train_indices)\n",
    "        self.val_sampler = SubsetRandomSampler(self.val_indices)\n",
    "        self.test_sampler = SubsetRandomSampler(self.test_indices)\n",
    "\n",
    "    def get_train_split_point(self):\n",
    "        return len(self.train_sampler) + len(self.val_indices)\n",
    "\n",
    "\n",
    "    def get_validation_split_point(self):\n",
    "        return len(self.train_sampler)\n",
    "\n",
    "\n",
    "    def get_split(self):\n",
    "        print('Initializing train-validation-test dataloaders')\n",
    "        self.train_loader = self.get_train_loader(batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "        self.val_loader = self.get_validation_loader(batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "        self.test_loader = self.get_test_loader(batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "        return self.train_loader, self.val_loader, self.test_loader\n",
    "    \n",
    "\n",
    "    def get_train_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing train dataloader')\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.dataset, sampler=self.train_sampler, shuffle=False, **kwargs)\n",
    "        return self.train_loader\n",
    "\n",
    "\n",
    "    def get_validation_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing validation dataloader')\n",
    "        self.val_loader = torch.utils.data.DataLoader(self.dataset, sampler=self.val_sampler, shuffle=False, **kwargs)\n",
    "        return self.val_loader\n",
    "\n",
    "\n",
    "    def get_test_loader(self, batch_size=50, num_workers=4):\n",
    "        logging.debug('Initializing test dataloader')\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.dataset, sampler=self.test_sampler, shuffle=False, **kwargs)\n",
    "        return self.test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cell_id": "ea16edf32a204d688f3aba11bb757c1b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 193,
    "execution_start": 1676554400524,
    "source_hash": "56ff1282",
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'is_cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Loading the data into three different sets\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train \u001b[38;5;241m=\u001b[39m B\u001b[38;5;241m.\u001b[39mget_train_loader()\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis train on GPU ? \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain\u001b[38;5;241m.\u001b[39mis_cuda\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m )\n\u001b[0;32m      7\u001b[0m test \u001b[38;5;241m=\u001b[39m B\u001b[38;5;241m.\u001b[39mget_test_loader()\n\u001b[0;32m      8\u001b[0m val \u001b[38;5;241m=\u001b[39m B\u001b[38;5;241m.\u001b[39mget_validation_loader()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'is_cuda'"
     ]
    }
   ],
   "source": [
    "normal_images = torchvision.datasets.ImageFolder(root=DATA_PATH, transform=transforms.ToTensor())\n",
    "B = DataSplit(normal_images, shuffle=True)\n",
    "\n",
    "# Loading the data into three different sets\n",
    "train = B.get_train_loader()\n",
    "# print(f\"is train on GPU ? {train.is_cuda}\" ) doesn't work\n",
    "test = B.get_test_loader()\n",
    "val = B.get_validation_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cell_id": "7dc94cc10dd84d1297f0cbc740e881cd",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 937,
    "execution_start": 1676554404301,
    "source_hash": "b568c63d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#loading the three models \n",
    "mod = models.resnet18(pretrained=True).to(device=device)\n",
    "num_ftrs = mod.fc.in_features\n",
    "mod.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "mod_green = models.resnet18(pretrained=True).to(device=device)\n",
    "num_ftrs_green = mod_green.fc.in_features\n",
    "mod_green.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "mod_skelet = models.resnet18(pretrained=True).to(device=device)\n",
    "num_ftrs_skelet = mod_skelet.fc.in_features\n",
    "mod_skelet.fc = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cell_id": "fcec91eed9de4d98886070d7db708f4a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1676554410727,
    "source_hash": "34efac97",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#la var str est le nom qu'on atribue les données générés par la pipeline\n",
    "#contiennent cette var afin de les retrouver\n",
    "normal = (\"normal\", mod, train, test, val)\n",
    "\n",
    "#permet d'éviter de rentrer des tuples à la main\n",
    "#Avec une boucle for on passe on passe automatiquement\n",
    "#les tuples dans la pipeline\n",
    "features = [normal]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "8ac1e6183b4048619eb544b5edab7a6b",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# Pipeline d'entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cell_id": "0fe3ec3a82fe43f3b5a417db23da5969",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 153,
    "execution_start": 1676554416167,
    "source_hash": "abec7c2f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class our_pipeline:\n",
    "    def __init__(self,feature, num_epochs:int, early_stopping_threshold:int):\n",
    "        \n",
    "        self.net = feature[0]\n",
    "        self.model = feature[1]\n",
    "        self.train = feature[2]\n",
    "        self.val = feature[3]\n",
    "        self.test = feature[4]\n",
    "\n",
    "        self.num_epochs = num_epochs\n",
    "        self.early_stopping_threshold = early_stopping_threshold\n",
    "        self.nb_class = len(self.train.dataset.classes)\n",
    "\n",
    "        self.optimizer = Adam(self.model.parameters())\n",
    "        #self.scheduler = CyclicLR(self.optimizer, base_lr=0.001, max_lr=0.01, step_size=10, mode='triangular')\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.scheduler = StepLR(self.optimizer, step_size=10)\n",
    "\n",
    "    def performances(self, data):\n",
    "\n",
    "        #les métriques déjà programmées ne conviennent pas on crée les notres\n",
    "        #Calcul de la sensibilité et de la spécificité\n",
    "        nb_correct_pred = [0] * self.nb_class\n",
    "        nb_total_pred = [0] * self.nb_class\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data:\n",
    "                outputs = self.model(inputs)\n",
    "                #Sorti du model liste de proba d'appartenir à une classe\n",
    "                #pred = argmax de la liste\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                for i in range(len(labels)):\n",
    "                    if labels[i] == predicted[i]:\n",
    "                        nb_correct_pred[labels[i]] += 1\n",
    "                    nb_total_pred[labels[i]] += 1\n",
    "                \n",
    "            self.good_pred = [nb_correct_pred[i] / nb_total_pred[i] for i in range(self.nb_class)]\n",
    "        return self.good_pred\n",
    "\n",
    "    def training(self):\n",
    "        self.model.train()\n",
    "        for inputs, labels in self.train:\n",
    "            #réinitialisation du gradient\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def row(self, epoch):\n",
    "        #Pour chaque epoch on veut avoir l'historique des performances sur train\n",
    "        #et sur val ==> 1 ligne de data frame = perf d'1 epoch\n",
    "        train_perf = self.performances(self.train)\n",
    "        val_perf = self.performances(self.val)\n",
    "\n",
    "        # balanced acc = (sepecificité + sensibilité)/2\n",
    "        self.bal_acc_val = val_perf[0]+val_perf[1]/self.nb_class\n",
    "        self.bal_acc_train = train_perf[0]+train_perf[1]/self.nb_class\n",
    "\n",
    "        self.newrow = pd.DataFrame({'mod': [self.net], 'epoch': [epoch],\n",
    "                            'true_neg_train':[train_perf[0]], 'true_pos_train':[train_perf[1]],\n",
    "                            'true_neg_val':[val_perf[0]], 'true_pos_val':[val_perf[1]],\n",
    "                            'balanced_acc_train': [self.bal_acc_train],\n",
    "                            'balanced_acc_val': [self.bal_acc_val]\n",
    "                            },index=[0])\n",
    "        \n",
    "        return self.newrow, self.bal_acc_val\n",
    "        # on renvoi self.bal_acc_val car on s'en sert pour arrêter l'entrainement\n",
    "        #En cas de surapprentissage\n",
    "\n",
    "    def training_loop(self, print_res : bool):\n",
    "\n",
    "        counter = 0\n",
    "        best_acc = 0.0\n",
    "\n",
    "        col_names = ['mod', 'epoch', 'true_neg_train', 'true_pos_train',\n",
    "                        'true_neg_val', 'true_pos_val', 'balanced_acc_train',\n",
    "                        'balanced_acc_val']\n",
    "\n",
    "        stats = pd.DataFrame(columns=col_names)\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.training()\n",
    "            perfs = self.row(epoch)\n",
    "            if print_res : \n",
    "                print(\"state\", self.net, \": \", epoch, \"/\", self.num_epochs)\n",
    "                print(perfs[0])\n",
    "            #A chaque époque on ajoute les stats\n",
    "            stats = pd.concat([stats, perfs[0]])\n",
    "\n",
    "            if perfs[1] > best_acc:\n",
    "                best_acc = perfs[1]\n",
    "                best_mod = self.model.state_dict()\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "\n",
    "            if counter >= self.early_stopping_threshold:\n",
    "                if print_res : \n",
    "                    print(\"Early stopping at epoch: \", epoch+1)\n",
    "                break\n",
    "        #Sauvegarde du model avec l'heure\n",
    "        now = time.time()\n",
    "        mod_save_path = f\"/work/{now}-Models_history/\" + self.net + \".pt\"\n",
    "        torch.save(best_mod, mod_save_path)\n",
    "\n",
    "        #Sauvegarde de l'historique avec l'heure\n",
    "        stats_save_path = \"/work/{now}-Models_history/\" + self.net + \".csv\"\n",
    "        stats.to_csv(stats_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cell_id": "56632825b17f43ca8e3cb10ca281aa48",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 5613151,
    "execution_start": 1676554423738,
    "source_hash": "93d5a003",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m tqdm (features, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(features)) :\n\u001b[0;32m      2\u001b[0m     C \u001b[38;5;241m=\u001b[39m our_pipeline(feature, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprint_res\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 84\u001b[0m, in \u001b[0;36mour_pipeline.training_loop\u001b[1;34m(self, print_res)\u001b[0m\n\u001b[0;32m     81\u001b[0m stats \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39mcol_names)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     perfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow(epoch)\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m print_res : \n",
      "Cell \u001b[1;32mIn[29], line 46\u001b[0m, in \u001b[0;36mour_pipeline.training\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m#réinitialisation du gradient\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 46\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, labels)\n\u001b[0;32m     48\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Torch_GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Torch_GPU\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Torch_GPU\\lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Torch_GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Torch_GPU\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Torch_GPU\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "for feature in tqdm (features, total = len(features)) :\n",
    "    C = our_pipeline(feature, 200, 20)\n",
    "    C.training_loop(print_res = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "852e70c162c14ad8b451931f46684cd1",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# Évaluation qualitative du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1cf50f067eea4bf68a20e72c8b157118",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1675762595781,
    "source_hash": "c236c3d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getSaliency(model,img,label):\n",
    "    model.eval()\n",
    "    #img = img.to(device)\n",
    "    img.requires_grad = True    # ajout du gradient sur l'image elle-même\n",
    "                                # idée: + gd gradient = + fort impact sur la décision\n",
    "    img.grad = None\n",
    "    outputs = nn.Softmax(dim=1)(model(img.unsqueeze(0)))    # ATTENTION, le softmax est dans la loss, pas dans le réseau \n",
    "                                                            # => il faut le remettre à la main pour que ça marche\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    output=outputs[0,label]     # focalisation sur la vérité terrain\n",
    "    output.backward()           # calcul des gradients associés à cette classe seulement\n",
    "    sal=img.grad.abs()          # récupération du gradient sur l'image (+abs)\n",
    "    if sal.dim()>2:\n",
    "        sal=torch.max(sal,dim=0)[0]\n",
    "    \n",
    "    print(\"True label :\", label.item(), \"Predicted label : \", predicted.item())\n",
    "    fig=plt.figure(figsize=(8, 8))\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(img.detach().cpu().permute(1,2,0),cmap=\"gray\")\n",
    "\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(sal.to('cpu'),cmap=\"seismic\",interpolation=\"bilinear\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7420b6086ad6489a8ba00ef767b10a11",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 191,
    "execution_start": 1675759307101,
    "source_hash": "5381c79b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18()\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "state_dict = torch.load(\"/work/Models_history/normal.pt\")\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "e93a6b522c2e45ca870ec09be415b333",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 771,
    "execution_start": 1675762598358,
    "source_hash": "7683a455",
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [51], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m val:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(labels)):\n\u001b[0;32m----> 5\u001b[0m         \u001b[43mgetSaliency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         nb \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m nb \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m20\u001b[39m:\n",
      "Cell \u001b[0;32mIn [50], line 19\u001b[0m, in \u001b[0;36mgetSaliency\u001b[0;34m(model, img, label)\u001b[0m\n\u001b[1;32m     17\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m explainer \u001b[38;5;241m=\u001b[39m lime\u001b[38;5;241m.\u001b[39mlime_image\u001b[38;5;241m.\u001b[39mLimeImageExplainer()\n\u001b[0;32m---> 19\u001b[0m explanation \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mexplain_instance(\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy(), predicted, top_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, hide_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     20\u001b[0m temp, mask \u001b[38;5;241m=\u001b[39m explanation\u001b[38;5;241m.\u001b[39mget_image_and_mask(explanation\u001b[38;5;241m.\u001b[39mtop_labels[\u001b[38;5;241m0\u001b[39m], positive_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, hide_rest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue label :\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted label : \u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "nb = 0\n",
    "while nb < 20:\n",
    "    for inputs, labels in val:\n",
    "        for i in range(len(labels)):\n",
    "            getSaliency(model,inputs[i],labels[i])\n",
    "            nb += 1\n",
    "            if nb == 20:\n",
    "                break\n",
    "        if nb == 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b90ef8a7485e473488f048b9b1a3a1af",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": [],
    "is_collapsed": false,
    "tags": []
   },
   "source": [
    "# Reconstitution de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "4cc8b9d0cfa04f1ca4b66b5db0627f9c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1676463408295,
    "source_hash": "85e4ee97",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def global_evaluation (img_path:str, model):\n",
    "\n",
    "    #Division de la grande image en sous-images\n",
    "    sushi = SubimageCreator(img_path, size=(100, 100))\n",
    "    sushi.cut()\n",
    "\n",
    "    #Chaque sous-image passe par le model pour être classifiée\n",
    "    for subimage in sushi.subimages:\n",
    "        model.eval()\n",
    "        outputs = nn.Softmax(dim=1)(model(img.unsqueeze(0)))\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        \n",
    "        #S'il y a un cirse sur l'imagette\n",
    "        #on la colore en rouge, sinon\n",
    "        #on la colore en vert\n",
    "        if prediction == 0:\n",
    "            col = 60\n",
    "        else:\n",
    "            col = 0\n",
    "            \n",
    "        path=sushi.output_dir+\"/\"+subimage\n",
    "        img = cv2.imread(path)\n",
    "        image_hued=change_hue(img,randint(10,col))\n",
    "        cv2.imwrite(sushi.output_dir+\"/\"+\"hued_\"+subimage,image_hued)\n",
    "    sushi.rebuild(\"hued_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "e5cadfce52ec444d881936ebf5282021",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1676463411353,
    "source_hash": "d4fbe88e",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m global_evaluation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDJI_0202.JPG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmodel\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "path = \"DJI_0202.JPG\"\n",
    "global_evaluation(path, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=6f211ba4-8439-4b4f-a72d-48759d094385' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "b2f31843c3f848ea86e694435331b3db",
  "kernelspec": {
   "display_name": "Torch_GPU kernel",
   "language": "python",
   "name": "torch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
